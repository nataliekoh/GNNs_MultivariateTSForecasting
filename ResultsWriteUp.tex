\documentclass[8pt]{extarticle}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Multivariate Time Series Forecasting with Graph Neural Networks
    }
    \author{Natalie Koh, Zachary Laswick, Daiwei Shen}
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=0.5in,bmargin=0.5in,lmargin=0.2in,rmargin=0.2in}
    
    

\begin{document}
    
    \maketitle
    
    %\hypertarget{multivariate-time-series-forecasting-with-graph-neural-networks}{%
%\section{\texorpdfstring{\textbf{Multivariate Time Series Forecasting
%with Graph Neural
%Networks}}{Multivariate Time Series Forecasting with Graph Neural Networks}}\label{multivariate-time-series-forecasting-with-graph-neural-networks}}

%Natalie Koh, Zachary Laswick, Daiwei Shen

    \hypertarget{introduction}{%
\subsubsection{Introduction}\label{introduction}}

    For this project, we explored the use of graph neural network (GNN)
architectures for multivariate time series forecasting. Specifically, we
wanted to use state of the art time series forecasting GNNs to predict
the future sensoring readings for a movement given the historical
readings. GNNs, which have only recently gained traction within the deep
learning field, are uniquely suited for solving time series forecasting
problems that require the consideration of spatial dependencies between
nodes because they can capture complex spatial and temporal
relationships between objects.

We had initially set out to compare two state-of-the-art GNNs ---
Recurrent Graph Evolution Neural Network (ReGENN) and ReGENN combined
with a pre-training model known as
\href{https://arxiv.org/pdf/2206.09113}{STEP}, as outlined in our
project proposal. STEP (STGNN is Enhanced by a scalable time series
Pre-training model) is a recently developed pre-processing framework
that involves pre-training a model to learn temporal patterns from
long-term history time series. This unsupervised pre-training model
generates segment-level representations using a transformer-like
architecture, which can be fed as inputs into a downstream GNN to boost
performance. In the original paper that proposed STEP, the authors used
their pre-training model with
\href{https://arxiv.org/pdf/1906.00121}{Graph WaveNet}, a GNN that uses
graph convolution to learn hidden spatial dependencies automatically
from data. Both STEP and Graph WaveNet were originally formulated to
predict traffic flow on complex road networks.

As we soon found out after starting on the project, the pairing of STEP
with a downstream GNN requires some pretty significant changes to the
downstream GNN in order to fuse the two models (see section 3.2 of STEP
paper). Due to time constraints and the complexity of implementation, we
decided to do away with ReGENN and focus on the implementation of STEP
in combination with Graph WaveNet instead. We thus aimed to assess the
performance of STEP with Graph WaveNet against using Graph WaveNet alone
on two novel datasets that these architectures were not benchmarked on.

Given the eventual difficulties encountered in getting Graph WaveNet to
work with our datasets (explained below), we have also included an
implemention of a simple graph convolutional network (GCN) with a LSTM
layer in Keras to also compare against the performance of STEP with
Graph WaveNet.

Ultimately, we quantified the performance of these architectures by
examining the ability of these GNNs to \textbf{forecast a single time
series given the history of a multivariate time series dataset}.

All code for this project can be found at our
\href{https://github.com/nataliekoh/GNNs_MultivariateTSForecasting}{GitHub}
repository.

    \hypertarget{datasets}{%
\subsubsection{Datasets}\label{datasets}}

    We tested the above GNNs on two physiological multivariate time series
datasets.

The first is the MHEALTH (Mobile HEALTH) dataset from UC Irvine, which
comprises body motion and vital signs recordings for 10 volunteers of
diverse profile while performing several physical activities. Sensors
placed on the subject's chest, right wrist and left ankle were used to
measure the motion experienced by diverse body parts -- namely,
acceleration, rate of turn and magnetic field orientation.

The second is the MotionSense (MSense) dataset from Kaggle, which
includes time series data generated by accelerometer and gyroscope
sensors (attitude, gravity, userAcceleration, and rotationRate)
collected from an iPhone 6s kept in the participant's front pocket. This
data was collected from 24 participants who performed various activities
such as walking, jogging, sitting, and standing.

In the traffic flow forecasting datasets that STEP and Graph WaveNet
were tested on, multiple sensors on different roads captured the same
information, providing a {[}\emph{time, num\_sensors, num\_features}{]}
dataset.

For our datasets, however, each sensor on a participant collected
separate information from other sensors. We thus decided to work with
data from a single randomly chosen subject from each dataset. For each
of these subjects, we selected five features (giving us five time
series). Hence, for both the MHealth and MSense datasets, we used as
input into our GNNs a {[}\emph{time, 1, num\_features}{]} tensor, and
data was split into training, validation, and test datasets using the
ratios 0.6, 0.2, and 0.2 respectively. We fed into our models 50 history
timesteps to forecast 50 future timesteps, and used a
{[}\emph{num\_features, num\_features}{]} identity matrix as the
adjacency matrix because actual distances between sensors were not
provided for either dataset.

Below we see the time series traces for a randomly chosen subject from
the MHealth dataset, and the time series traces for another randomly
chosen subject from the MSense dataset.

\includegraphics[scale=0.5]{mhealth_subject1_traces.png}
\includegraphics[scale=0.5]{msense_subject10_traces.png}

    \hypertarget{step-framework}{%
\subsubsection{STEP Framework}\label{step-framework}}

    As introduced above, STEP is a recently developed framework that
combines model pre-training with a GNN for time-series forecasting. For
the STEP Framework, significant portions of the code that the authors
made available on GitHub had to be adjusted to handle the data used for
inputs -- namely the number of nodes within the GNN, reshaping of the
model's final output, and the input parameters to handle the historical
and future sequence length of 50. Due to STEP's structure, the number of
nodes within the model should be equal to the number of features, which
for our data was 5. Additionally, the output of the STEP model contained
a bug that resulted in the addition of a dimension with a shape of 1
into the output when used with new data, but this bug was able to be
resolved by reshaping the output to reduce this dimension down. Finally,
the initial parameters, including the input length, steps, patch size,
and out channel, had to be adjusted to enable the model to take an input
with time length of 50 and predict a time length of 50.

To train our modified STEP model, we used an ADAM optimizer with a
learning rate of 0.005 and weight decay of 1e-5, with a dropout layer of
0.1 frequency and an encoder depth of 4 with decoder depth of 1. Using
the model on our novel datasets, we were able to obtain a test loss of
roughly 0.5 and 0.15 and a validation loss of roughly 3.0 and 0.4 for
MHealth and MSense respectively, as seen in the figures below. This loss
steadily decreased, but only very slowly, with increasing epochs.

However, since the prediction for the best loss barely resembles the
true future values for a feature as seen in the images, the STEP model
for this data did not perform as well as expected. Some of the
challenges in using this model apart from the decreased performance than
expected, include the inability of the model to handle single subject
datasets, hardcoding by the authors, and the limited nuber of nodes
used, since the node number equaled the feature number, which was only
5. The biggest challenge was the hardcoding done by the authors, where
they had embedded numerous parameters within various files, each of
which needed to be adjusted to handle new data and needed to be
specified for each dataset. This hardcoding resulted in eight files that
needed to be significantly modified for the model to work with our
data.

\includegraphics[scale=0.5]{stepimage1.png}
\includegraphics[scale=0.5]{stepimage2.png}
\includegraphics[scale=0.5]{stepimage3.png}
\includegraphics[scale=0.5]{stepimage4.png}

    \hypertarget{graph-wavenet}{%
\subsubsection{Graph WaveNet}\label{graph-wavenet}}

    Graph WaveNet is a GNN designed to address some of the shortcomings of
traditional architectures, such as CNNs and RNNs, when it comes to time
series forecasting. In particular, the architecture emphasizes the usage
of an adjacency matrix, as well as convolution based on exponentially
growing its kernel dimensions with respect to the number of layers. The
authors of this neural network found that these components allowed it to
learn long time-series sequences, as well as spatial-temporal
dependencies, on a set of traffic data. Below, we test the model's
performance on two medical datasets and visualize the results.

Below we illustrate the model's performance on the MHealth dataset. This
model was trained using the following parameters: Learning rate = .0001;
Number of nodes = 1; Input dim = 5; Seq length = 12; Batch size = 64;
Epochs = 50; Drop out probability = .2. We used a sequence length 12 because we observed during our testing that this model produces particularly
poor results when using a sequence length other than 12. Graph WaveNet was also not able to handed unequal history or forecast sequence lengths. We suspect that this is because
the backend of the model was heavily hard-coded by the authors. 

The time-series forecast performance on the features of subject 1 in our
dataset is visualized as a single plot of 12 time steps.

\includegraphics[scale=0.5]{wavenet_fig1.png}
\includegraphics[scale=0.5]{wavenet_fig2.png}

For the MSense dataset, we used the following parameters: Learning rate
= .001; Number of nodes = 24; Input dim = 8; Seq length = 12; Batch size
= 64; Epochs = 100; Drop out probability = .2. 

The following plots illustrate the results of the model for the MSense
dataset. The time-series forecast performance on the features of subject
10 in our dataset is visualized as a single plot of 12 time steps.

\includegraphics[scale=0.5]{wavenet_fig3.png}
\includegraphics[scale=0.5]{wavenet_fig4.png}

We find that the performance of Graph WaveNet on MSense is acceptable,
but not spectacular. We do observe clear improvement over training
epochs, but the gains are small, particularly when it comes to
validation loss. We could not get the model to achieve loss lower than
about .10. Despite these shortcomings, the results obtained in our
testing do reflect untapped potential in this architecture. Based on the
predictive performance shown, it is clear that the model is learning the
spatial relations in the input data. We believe that given more time,
additional modifications and optimizations could be made to the model to
improve its performance, particularly when it comes to the MSense
dataset.

Although both the training and validation losses decrease sharply at the start, they both hit a wall at around .5 and .7, respectively, and do not improve further with additional epochs. For this reason, we set the number of epochs to just 50. Adjusting the parameters did not lead to significant improvement in performance. Needless to say, the performance of WaveNet on the MHealth data is extremely poor. We observe that the forecasted time-series curve does not remotely resemble the target curve. It is difficult to pinpoint the reason for the model's performance discrepancy between the MHealth and MSense datasets. Our results suggest that the model may achieve significantly higher performance certain datasets than others.

Indeed,
we ran into multiple obstacles involving code adaptability. For
instance, the model only accepts data structured as a 4 dimensional
tensor in the following way: \emph{(samples, seq\_length, nodes,
features)}. This data structure was not ideal for the datasets we used,
but we made do by mapping analogical categories between our data and
that the authors' used (e.g.~instead of nodes on dimension 3, we had
subjects in the MSense data). Another challenge we ran into was that we
could not use the proposed adaptive adjacency matrix because the authors
had hard coded it for their traffic data. Therefore, we opted for the
identity matrix, which is one of the alternatives discussed in the
research article.

    \hypertarget{simple-gcn-with-lstm-layer}{%
\subsubsection{Simple GCN with LSTM
Layer}\label{simple-gcn-with-lstm-layer}}

    Given the challenges we faced in getting Graph WaveNet to work, we built
a simple model in Keras comprising a graph convolution layer coupled
with a LSTM layer that is capable of capturing both spatial and temporal
information in time series data to compare our results to. For this
model, we used a batch size of 64, with 64 LSTM units for the LSTM
layer.

As we can see from the results, the model did a mediocre job of
forecasting the 1st feautre for each dataset. For the MHealth dataset,
validation loss remained high throughout training while training loss
decreased slowly and only by a small amount, indicating that the model
was not learning to predict the time series features very well. More
significant reductions in validation and training loss were observed for
the MSense dataset, but the loss error remained high at 20 epochs. In
both cases, the model seemed to do a decent job of fitting the means of
the time series, but failed to capture their variability.

\includegraphics[scale=0.5]{mhealth_subject1_lossoverepochs_GCN.png}
\includegraphics[scale=0.5]{mhealth_subject1_predictedfeature1_GCN.png}
\includegraphics[scale=0.5]{msense_subject10_lossoverepochs_GCN.png}
\includegraphics[scale=0.5]{msense_subject10_predictedfeature1_GCN.png}

    \hypertarget{conclusions}{%
\subsubsection{Conclusions}\label{conclusions}}

Based on our results, the STEP framework performed the worst on both our datasets, while the simple GCN with LSTM layer did the best for the MHealth dataset and Graph WaveNet alone did the best for the MSense dataset.

This is surprising and disappointing in light of STEP's performance on
the traffic datasets it was benchmarked on.

As explained above, we faced faced numerous challenges in implementing
both STEP and Graph WaveNet, and wrangling with the code base that was
provided by the authors of these models (for example, Graph WaveNet's
codebase was unmaintained and had several issues flagged on their GitHub
page that were not addressed by the authors). We also found that
numerous parameters and design components were hardcoded into the
architectures to deal specifically with traffic datasets.

Other factors that could have contributed to the poor performance of
STEP and Graph WaveNet include the fact that we were testing these
models on physiological data that are inherently noisy. In addition, we
were not able to fully leverage the spatial modeling abilities of these
GNNs because we were not able to obtain information regarding the actual
or functional distances between sensors.

We note that STEP or Graph WaveNet may work better if it was tested on
more than two subjects and longer historical sequences are used since
this has been shown to improve the forecasting of other types of neural
networks.

\end{document}
